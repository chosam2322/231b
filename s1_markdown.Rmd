---
title: "231B Section 1"
author: "Chris Carter \\ https://github.com/unc421/231B"
date: "January 21, 2016"
output:
  slidy_presentation:
      fig_height: 5
      font_adjustment: +1
---

```{r, echo=FALSE}
# libraries
library(png)
library(grid)
library(ggplot2)
# wd
setwd("~/Dropbox/231B_Spring2015/231b_section_materials_2015/section_1")
```


Today
=====================================================================
1. Overview of sections
2. Simulations in R: the sample mean
3. Functions in R: the difference of two means


Sections
=====================================================================

## Two goals:
- Illustrating and understanding the concepts covered in lecture
- Going from learning R to programming in R

[Six steps to a better relationship with your future self.](http://polmeth.wustl.edu/methodologist/tpm_v18_n2.pdf)

For some sections, we will be using codeshare: http://www.codeshare.io/NQ7Gn

We will be writing simulations and functions at least 80% of our time - some material to revisit the basics:

* Functions: [General thought on what makes a good function](http://nicercode.github.io/guides/functions/), [beginner](http://nicercode.github.io/intro/writing-functions.html), [intermediate/advanced](http://adv-r.had.co.nz/Functions.html). Shalizi on functions, [part 1](http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/06/lecture-06.pdf) & [part 2](http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/07/lecture-07.pdf).
* Simulations: [Repeating things](http://nicercode.github.io/guides/repeating-things/), [flow control and looping](http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/03/lecture-03.pdf).


Problem Sets
=====================================================================
- Send a digital copy to guadalupe.tunon+231b@gmail.com
- *Compiled versions* of your code. ".R" files will not be considered for grading. 
- Use [sweave, knitr](https://support.rstudio.com/hc/en-us/articles/200552056-Using-Sweave-and-knitr) or [R markdown](http://rmarkdown.rstudio.com/pdf_document_format.html)
- When debugging your code, [you might find Cosma Shalizi's advice helpful](http://www.stat.cmu.edu/~cshalizi/statcomp/14/lectures/13/lecture-13.pdf)
- Practice [defensive programming](http://software-carpentry.org/v5/novice/python/05-defensive.html)
- Problem sets due at 12 mignight on Wednesdays


----

Why program? (Shalizi)
=====================================================================

- **Independence**: Otherwise, you rely on someone else having given you exactly the right tool

- **Honesty**: Otherwise, you end up distorting your problem to match the tools you have

- **Clarity**: Making your method something a machine can do disciplines your thinking and makes it public

> Programs should be written for people to read, and only incidentally for
machines to execute (Abelson and Sussman)

- Machines are made out of machines; functions are made out of functions. The route to good programming is to take the big transformation and break it down into smaller ones, and then break those down, until you come to tasks which the built-in functions can do


2. A simulation in R: sample mean as an unbiased estimator of the population mean
======================================================================

First we will need to "create" a population, a box of tickets

```{r}
population <- c(4,5,7,12,7,8,9,-3,5,8,9,3,2,3,4,6,10,4,6,7,8,9,2)

N <- length(population) # number of observations in the population
N

pop_mean <- mean(population) # population mean
pop_mean 

pop_sd <- sd(population) # population standard deviation
pop_sd
```

----

We will draw several random samples of 8 observation without replacement 
```{r}
s1 <- sample(population, size=8, replace = FALSE)

s2 <- sample(population, size=8, replace = FALSE)

s3 <- sample(population, size=8, replace = FALSE)

s4 <- sample(population, size=8, replace = FALSE)

samples <- rbind(s1, s2, s3, s4)

samples
```

----

Remember the population mean: `r pop_mean`

And the means of the samples 

```{r} 
apply(samples, MARGIN=1, FUN=mean) 
```

By chance each given sample mean may be a little higher or lower than the population mean. 

How can we use R to show that the sample mean is an unbiased estimator of the population mean?

----

For this, we will write a simulation. We will repeat the sample process 10,000 times.

```{r}

sample_mean <- NA

for (i in 1:10000){
  
  sample <- sample(population, size=8, replace = FALSE)
  sample_mean[i] <- mean(sample)
  
}
```

----

```{r}
par(mfrow=c(1,1))
plot(density(sample_mean), col="blue", lwd=3,
     main="Distribution of sample means")
abline(v=pop_mean, col="red", lwd=2)

```

Let's now look at the distribution of the sample mean as m gets closer to N.
======================================================================

So far, $m=8$. We now need a new simulation that adds a new step: we need to vary the size of m.

----

```{r, eval=FALSE}

rep <- 10000

# The first loop varies m
for (m in 9:20){

  sample_mean <- NA #creating an object to store the results of the second loop
  
  # The second loop goes through the 10000 simulations
  for (i in 1:rep){
      
    #we first get a random sample of size m from the population
    sample <- sample(population, size=m, replace = FALSE)
    #and then calculate and store the sample mean
    sample_mean[i] <- mean(sample)
  }
  
  #finally, we plot the distribution of the 10000 sample means for the relevant m
  lines(density(sample_mean), lwd=3,
        #note that this next line of code varies the color of the line according to m 
        #so that we can distinguish the different distributions
        col=paste0("grey",140-(7*m)))
}

```

What do we expect? Why?

----

```{r, echo=FALSE}

plot(density(sample_mean), col="blue", ylim=c(0,1.6),
     main="Distribution of sample means", lwd=3)
abline(v=pop_mean, col="red", lwd=3)

rep <- 10000

for (m in 9:20){
  sample_mean <- NA
  
  for (i in 1:rep){
    sample <- sample(population, size=m, replace = FALSE)
    sample_mean[i] <- mean(sample)
  }
  
  lines(density(sample_mean), lwd=3,
        col=paste0("grey",140-(7*m)))
}

```


3. Writing functions in R: the difference of two means
======================================================================

```{r}
diff_means <- function(y, x){ 
  
  # Calculating difference in means
  mean1 <- mean(y[x==1], na.rm=T)
  mean0 <- mean(y[x==0], na.rm=T)
  diff <- mean1 - mean0
  
  # Calculating number of observations
  N <- length(na.omit(y))
  
  # Preparing output
  res <- c(mean1, mean0, diff, N)
  names(res) <- c("Mean 1", "Mean 0", "Difference", "N")
  
  return(c(res))
}
```

----

To try our function, we will use the small dataset in Gerber & Green (2012)
```{r}
gg_data <- as.data.frame(cbind(c(10,15,20,20,10,15,15), 
                               c(15,15,30,15,20,15,30)))
names(gg_data) <- c("Y_i0", "Y_i1")
save(gg_data, file="gg_data.Rda")
```

(`"gg_data.Rda"` uploaded on bcourses and in the github repository)

----

We will need to "create" a treatment vector...
```{r}
# let's fix m=3 (units in the treatment group)
treat <- c(1, 1, 1, 0, 0, 0, 0)
gg_data$treat <- sample(treat, 7, replace=F)
gg_data$treat
```

...and a column with the "observed" outcomes
```{r}
gg_data$observed <- ifelse(gg_data$treat==1, gg_data$Y_i1, gg_data$Y_i0)
```

----

Let's see how the complete data set looks now:

```{r}
head(gg_data)
```
----
```{r}
# mean of the treatment group
mean(gg_data$observed[gg_data$treat==1])
# mean of the control group
mean(gg_data$observed[gg_data$treat==0])

# difference of means
mean(gg_data$observed[gg_data$treat==1]) - mean(gg_data$observed[gg_data$treat==0])
  
# with our function
diff_means(gg_data$observed, gg_data$treat)

```

----

How can we get a distribution of the difference of means?
==============================================

Working in groups, we will combine 2 and 3 to write a simulation that generates the distribution of the difference in means.

Let's think about the steps first. 

---

For each simulation, 

> - 1. We will need to "create" a random treatment vector and generate the column with the associated observed outcomes.

> - 2. We will have to calculate the difference between the treatment and control means (by hand or using our new function).

---

```{r}

# 1.
gg_data$treat <- sample(treat, 7, replace=F)
gg_data$observed <- ifelse(gg_data$treat==1, gg_data$Y_i1, gg_data$Y_i0)

# 2.
diff_means(gg_data$observed, gg_data$treat)
# we should store this! so,
dm <- diff_means(gg_data$observed, gg_data$treat)
dm
# but we only want the third element!
dm <- diff_means(gg_data$observed, gg_data$treat)[3]
dm

```

----

Now let's put this in a loop that allows us to repeat the process 10000 times (and saves the dom for each)..

```{r}

dm <- NA #creating a placeholder to store all our doms...

for (i in 1:10000){
    
    # 1.
    gg_data$treat <- sample(treat, 7, replace=F)
    gg_data$observed <- ifelse(gg_data$treat==1, gg_data$Y_i1, gg_data$Y_i0)
    
    # 2.
    dm[i] <- diff_means(gg_data$observed, gg_data$treat)[3]

    }

```

----

Finally, let's plot the distribution

```{r}
hist(dm, col="blue", main="Histogram of Difference of Means \n for GGdata")

```


----
Problem set 1
==============================================

- Simulation to show the difference of two means is unbiased
- Distribution of the difference of means as m changes
- Extend the difference of means function to calculate the SE of the difference (we will cover SEs in lecture next Tuesday)
- Replicate figures and tables in Dunning and Harrison (2010) -- you will need your new function for this!

